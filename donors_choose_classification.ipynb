{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import pylab as pl\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.sparse import hstack\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10001)\n",
    "import random\n",
    "import tensorflow as tf\n",
    "random.seed(10002)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=6, inter_op_parallelism_threads=5)\n",
    "from keras import backend\n",
    "tf.set_random_seed(10003)\n",
    "backend.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_raw_data=pd.read_csv(\"../../../Users/sreek/Documents/vg_donors_choose/train/train.csv\")\n",
    "test_raw_data=pd.read_csv(\"../../../Users/sreek/Documents/vg_donors_choose/test/test.csv\",low_memory=False)\n",
    "res_raw_data=pd.read_csv(\"../../../Users/sreek/Documents/vg_donors_choose/resources/resources.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182080, 16)\n",
      "(78035, 15)\n",
      "(1541272, 4)\n"
     ]
    }
   ],
   "source": [
    "print (train_raw_data.shape)\n",
    "print(test_raw_data.shape)\n",
    "print (res_raw_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Ehsan's kernel  was really handy while trying to feature engineer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanup_data(dataF):\n",
    "    \n",
    "    dataF.loc[dataF.project_essay_4.isnull(), ['project_essay_4','project_essay_2']] = \\\n",
    "    dataF.loc[dataF.project_essay_4.isnull(), ['project_essay_2','project_essay_4']].values\n",
    "\n",
    "    dataF[['project_essay_2','project_essay_3']] = dataF[['project_essay_2','project_essay_3']].fillna('')\n",
    "\n",
    "    dataF['project_essay_1'] = dataF.apply(lambda row: ' '.join([str(row['project_essay_1']), \n",
    "                                                     str(row['project_essay_2'])]), axis=1)\n",
    "    dataF['project_essay_2'] = dataF.apply(lambda row: ' '.join([str(row['project_essay_3']),\n",
    "                                                     str(row['project_essay_4'])]), axis=1)\n",
    "\n",
    "    dataF = dataF.drop(['project_essay_3', 'project_essay_4'], axis=1,inplace=True)\n",
    "\n",
    "\n",
    "cleanup_data(train_raw_data)\n",
    "cleanup_data(test_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_raw_data['Total'] = res_raw_data['quantity']*res_raw_data['price']\n",
    "res_data = res_raw_data.groupby('id').agg({'description':'count',\n",
    "                            'quantity':'sum',\n",
    "                            'price':'sum',\n",
    "                            'Total':'sum'}).rename(columns={'description':'items'})\n",
    "res_data['avgPrice'] = res_data.Total / res_data.quantity\n",
    "numFeatures = ['items', 'quantity', 'price', 'Total', 'avgPrice']\n",
    "\n",
    "for func in ['min', 'max', 'mean']:\n",
    "    res_data = res_data.join(res_raw_data.groupby('id').agg({'quantity':func,\n",
    "                                          'price':func,\n",
    "                                          'Total':func}).rename(\n",
    "                                columns={'quantity':func+'Quantity',\n",
    "                                         'price':func+'Price',\n",
    "                                         'Total':func+'Total'}).fillna(0))\n",
    "    numFeatures += [func+'Quantity', func+'Price', func+'Total']\n",
    "\n",
    "res_data = res_data.join(res_raw_data.groupby('id').agg(\n",
    "    {'description':lambda x:' '.join(x.values.astype(str))}).rename(\n",
    "    columns={'description':'resource_description'}))\n",
    "\n",
    "train_data = train_raw_data.join(res_data, on='id')\n",
    "test_data=test_raw_data.join(res_data,on='id')\n",
    "\n",
    "\n",
    "train_data['price_category'] = pl.digitize(train_data.Total, [0, 50, 100, 250, 500, 1000, pl.inf])\n",
    "test_data['price_category'] = pl.digitize(test_data.Total, [0, 50, 100, 250, 500, 1000, pl.inf])\n",
    "numFeatures.append('price_category')\n",
    "\n",
    "for c in ['Quantity', 'Price', 'Total']:\n",
    "    train_data['max%s_min%s'%(c,c)] = train_data['max%s'%c] - train_data['min%s'%c]\n",
    "    test_data['max%s_min%s'%(c,c)] = test_data['max%s'%c] - test_data['min%s'%c]\n",
    "    numFeatures.append('max%s_min%s'%(c,c))\n",
    "\n",
    "del res_data, train_raw_data, res_raw_data, test_raw_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the teachers have applied multiple times so getting agrregated features could help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data['teacher_id'] = LabelEncoder().fit_transform(train_data['teacher_id'])\n",
    "train_data['teacher_gender_unknown'] = train_data.teacher_prefix.apply(lambda x:int(x not in ['Ms.', 'Mrs.', 'Mr.']))  \n",
    "test_data['teacher_id'] = LabelEncoder().fit_transform(test_data['teacher_id'])\n",
    "test_data['teacher_gender_unknown'] = test_data.teacher_prefix.apply(lambda x:int(x not in ['Ms.', 'Mrs.', 'Mr.']))  \n",
    "aggFtrs=[]    \n",
    "       \n",
    "for col in ['school_state', 'teacher_id', 'teacher_prefix', 'teacher_gender_unknown', 'project_grade_category', \n",
    "                'project_subject_categories', 'project_subject_subcategories', 'teacher_number_of_previously_posted_projects']:\n",
    "        Aggtr = train_data[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_agg'})\n",
    "        Aggts = test_data[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_agg'})\n",
    "        Aggtr /= Aggtr.sum()\n",
    "        Aggts /= Aggts.sum()\n",
    "        train_data = train_data.join(Aggtr, on=col)\n",
    "        test_data = test_data.join(Aggts, on=col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numFeatures += ['teacher_number_of_previously_posted_projects','teacher_id','teacher_gender_unknown']\n",
    "aggFtrs+=['school_state_agg', 'teacher_id_agg', 'teacher_prefix_agg', 'teacher_gender_unknown_agg', \\\n",
    "          'project_grade_category_agg', \n",
    "          'project_subject_categories_agg', 'project_subject_subcategories_agg',\\\n",
    "          'teacher_number_of_previously_posted_projects_agg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on discussions over kaggle forum , day of the week could be an important factor. I wasn't sure if hour or days were going to be significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTimeFeatures(data):\n",
    "    data['year'] = data['project_submitted_datetime'].apply(lambda x: x.year)\n",
    "    data['month'] = data['project_submitted_datetime'].apply(lambda x: x.month)\n",
    "    data['day'] = data['project_submitted_datetime'].apply(lambda x: x.day)\n",
    "    data['dow'] = data['project_submitted_datetime'].apply(lambda x: x.dayofweek)\n",
    "    return data\n",
    "train_data['project_submitted_datetime'] = pd.to_datetime(train_data['project_submitted_datetime'])\n",
    "test_data['project_submitted_datetime'] = pd.to_datetime(test_data['project_submitted_datetime'])\n",
    "getTimeFeatures(train_data)\n",
    "getTimeFeatures(test_data)\n",
    "\n",
    "for col in  ['year', 'month', 'day', 'dow']:\n",
    "        Aggtr = train_data[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_agg'})\n",
    "        Aggts = test_data[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_agg'})\n",
    "        Aggtr /= Aggtr.sum()\n",
    "        Aggts /= Aggts.sum()\n",
    "        train_data = train_data.join(Aggtr, on=col)\n",
    "        test_data = test_data.join(Aggts, on=col)\n",
    "            \n",
    "\n",
    "aggFtrs+=['year_agg', 'month_agg', 'day_agg', 'dow_agg']\n",
    "numFeatures += ['year', 'month', 'day', 'dow']\n",
    "numFeatures += aggFtrs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_features = ['teacher_prefix','school_state','project_grade_category',\\\n",
    "                'project_subject_categories','project_subject_subcategories']\n",
    "cat_features_hash = [col+\"_hash\" for col in cat_features]\n",
    "max_size=15000\n",
    "def feature_hash(df, max_size=max_size):\n",
    "    for col in cat_features:\n",
    "        df[col+\"_hash\"] = df[col].apply(lambda x: hash(x)%max_size)\n",
    "    return df\n",
    "\n",
    "train_data = feature_hash(train_data)\n",
    "test_data = feature_hash(test_data)\n",
    "\n",
    "train_cat_new = np.array(train_data[cat_features_hash], dtype=np.int)\n",
    "test_cat_new = np.array(test_data[cat_features_hash], dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pprocess(string):\n",
    "\n",
    "    string = re.sub(r'(\\\")', ' ', string)\n",
    "    string = re.sub(r'(\\r)', ' ', string)\n",
    "    string = re.sub(r'(\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\r\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\\\)', ' ', string)\n",
    "    string = re.sub(r'\\t', ' ', string)\n",
    "    string = re.sub(r'\\:', ' ', string)\n",
    "    string = re.sub(r'\\\"\\\"\\\"\\\"', ' ', string)\n",
    "    string = re.sub(r'_', ' ', string)\n",
    "    string = re.sub(r'\\+', ' ', string)\n",
    "    string = re.sub(r'\\=', ' ', string)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_cols=['project_essay_1','project_essay_2','project_resource_summary','resource_description','project_title']\n",
    "train_data['text'] = train_data.apply(lambda x: \" \".join(x[col] for col in text_cols), axis=1)\n",
    "test_data['text'] = test_data.apply(lambda x: \" \".join(x[col] for col in text_cols), axis=1)                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['text']=train_data['text'].apply(pprocess)\n",
    "test_data['text']=test_data['text'].apply(pprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "porter = PorterStemmer()\n",
    "def filter_stop_words(sentence):\n",
    "    new_sent = [porter.stem(word.lower()) for word in sentence.split() if word not in stop_words]\n",
    "    new_sent=  [\"\".join(sent) for sent in new_sent]\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['text']=train_data['text'].apply(filter_stop_words)\n",
    "test_data['text']=test_data['text'].apply(filter_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 100000\n",
    "from keras.preprocessing import text,sequence\n",
    "tokenizer = text.Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "tokenizer.fit_on_texts(train_data['text'].str.join(' ').tolist()+test_data['text'].str.join(' ').tolist())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_text_data_list=tokenizer.texts_to_sequences(train_data[\"text\"].str.join(' ').tolist())\n",
    "ta_text_data_list=tokenizer.texts_to_sequences(test_data[\"text\"].str.join(' ').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_words = sequence.pad_sequences(tr_text_data_list, maxlen=300)\n",
    "test_words = sequence.pad_sequences(ta_text_data_list, maxlen=300)\n",
    "\n",
    "train_target = train_data.project_is_approved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../../../Users/sreek/Documents/vg_donors_choose/crawl-300d-2M.vec/crawl-300d-2M.vec'\n",
    "embed_size=300\n",
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten, concatenate, Dropout, Convolution1D, \\\n",
    "GlobalMaxPool1D,SpatialDropout1D,Bidirectional,PReLU,GRU\n",
    "from keras.models import Model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train_data_num=StandardScaler().fit_transform(train_data[numFeatures].fillna(0))\n",
    "test_data_num=StandardScaler().fit_transform(test_data[numFeatures].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_NN_model():\n",
    "    cat_input = Input((len(cat_features_hash), ))\n",
    "    num_input = Input((len(numFeatures), ))\n",
    "    text_input = Input((300, ))\n",
    "    m_cat=Embedding(max_size,10)(cat_input)\n",
    "    m_cat=SpatialDropout1D(0.3)(m_cat)\n",
    "    m_cat = Flatten()(m_cat)\n",
    "    m_words = Embedding(max_features, 300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)(text_input)\n",
    "    m_words = SpatialDropout1D(0.3)(m_words)\n",
    "    m_words =Bidirectional(GRU(50, return_sequences=True))(m_words)\n",
    "    m_words = Convolution1D(100, 3, activation=\"relu\")(m_words)\n",
    "    m_words = GlobalMaxPool1D()(m_words)\n",
    "    m_cat = Dense(100, activation=\"relu\")(m_cat)\n",
    "    m_num = Dense(100, activation=\"relu\")(num_input)\n",
    "    m = concatenate([m_cat, m_num, m_words])\n",
    "    m = Dense(50, activation=\"relu\")(m)\n",
    "    m = Dropout(0.25)(m)\n",
    "    predictions = Dense(1, activation=\"sigmoid\")(m)\n",
    "    model = Model(inputs=[cat_input, num_input, text_input], outputs=predictions)\n",
    "    model.compile(optimizer=optimizers.Adam(0.0005, decay=1e-6),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 163872 samples, validate on 18208 samples\n",
      "Epoch 1/5\n",
      "Epoch 00000: val_loss improved from inf to 0.37153, saving model to RNN.h5\n",
      "448s - loss: 0.4015 - acc: 0.8468 - val_loss: 0.3715 - val_acc: 0.8509\n",
      "Epoch 2/5\n",
      "Epoch 00001: val_loss improved from 0.37153 to 0.35445, saving model to RNN.h5\n",
      "442s - loss: 0.3717 - acc: 0.8514 - val_loss: 0.3545 - val_acc: 0.8573\n",
      "Epoch 3/5\n",
      "Epoch 00002: val_loss improved from 0.35445 to 0.34915, saving model to RNN.h5\n",
      "444s - loss: 0.3568 - acc: 0.8564 - val_loss: 0.3492 - val_acc: 0.8591\n",
      "Epoch 4/5\n",
      "Epoch 00003: val_loss improved from 0.34915 to 0.34860, saving model to RNN.h5\n",
      "442s - loss: 0.3507 - acc: 0.8593 - val_loss: 0.3486 - val_acc: 0.8591\n",
      "Epoch 5/5\n",
      "Epoch 00004: val_loss improved from 0.34860 to 0.34573, saving model to RNN.h5\n",
      "448s - loss: 0.3456 - acc: 0.8612 - val_loss: 0.3457 - val_acc: 0.8603\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "file_path='RNN.h5'\n",
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=2, save_best_only=True, save_weights_only=True,\n",
    "                                     mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=4)\n",
    "lr_reduced = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=2,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='min')\n",
    "callbacks_list = [checkpoint, early, lr_reduced]\n",
    "model=build_NN_model()\n",
    "history = model.fit([train_cat_new, train_data_num, train_words], train_target, validation_split=0.1,\n",
    "                    verbose=2,callbacks=callbacks_list,\n",
    "          epochs=5, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_cat_new, train_data_num, train_words,train_target\n",
    "md1=model.load_weights(file_path)\n",
    "pred_test = model.predict([test_cat_new, test_data_num, test_words], batch_size=2000)\n",
    "test[\"project_is_approved\"] = pred_test\n",
    "test[['id', 'project_is_approved']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "        'eta': 0.05,\n",
    "        'max_depth': 4,\n",
    "        'subsample': 0.85,\n",
    "        'colsample_bytree': 0.25,\n",
    "        'min_child_weight': 3,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'seed': 0,\n",
    "        'silent': 1,\n",
    "    }\n",
    "params_lgb = {\n",
    "        'boosting_type': 'dart',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.25,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'seed': 0,\n",
    "        'verbose': 0,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# boosting models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using numerical only columns to build an lgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's auc: 0.720402\tvalid_1's auc: 0.711514\n",
      "[400]\ttraining's auc: 0.728484\tvalid_1's auc: 0.713318\n",
      "[600]\ttraining's auc: 0.735897\tvalid_1's auc: 0.714247\n",
      "[800]\ttraining's auc: 0.744561\tvalid_1's auc: 0.714988\n",
      "[1000]\ttraining's auc: 0.754132\tvalid_1's auc: 0.715458\n",
      "[1200]\ttraining's auc: 0.762434\tvalid_1's auc: 0.715777\n",
      "Early stopping, best iteration is:\n",
      "[1155]\ttraining's auc: 0.760663\tvalid_1's auc: 0.715957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "for i in range(21, 22):\n",
    "    gc.collect()\n",
    "    lgb_train,lgb_val,lgb_tar_train,lgb_tar_val = train_test_split(train_data_num, train_target, test_size=0.15, random_state=i,\\\n",
    "                                                          stratify=train_target)\n",
    "    dtrain = lgb.Dataset(lgb_train, lgb_tar_train)\n",
    "    dval   = lgb.Dataset(lgb_val, lgb_tar_val)\n",
    "    lgb_model = lgb.train(params_lgb, dtrain, num_boost_round=10000, valid_sets=[dtrain, dval], early_stopping_rounds=200, verbose_eval=200)\n",
    "    lgb_val1 = lgb_model.predict(lgb_val)\n",
    "    lgb_pred= lgb_model.predict(train_data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using text only columns for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.590192\tvalid-auc:0.586312\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 200 rounds.\n",
      "[200]\ttrain-auc:0.690907\tvalid-auc:0.658928\n",
      "[400]\ttrain-auc:0.726801\tvalid-auc:0.665668\n",
      "[600]\ttrain-auc:0.754075\tvalid-auc:0.669026\n",
      "[800]\ttrain-auc:0.776889\tvalid-auc:0.670739\n",
      "[1000]\ttrain-auc:0.795703\tvalid-auc:0.672082\n",
      "[1200]\ttrain-auc:0.813264\tvalid-auc:0.674885\n",
      "[1400]\ttrain-auc:0.829417\tvalid-auc:0.675555\n",
      "[1600]\ttrain-auc:0.843203\tvalid-auc:0.675945\n",
      "[1800]\ttrain-auc:0.856291\tvalid-auc:0.676175\n",
      "[2000]\ttrain-auc:0.868456\tvalid-auc:0.67654\n",
      "[2200]\ttrain-auc:0.879033\tvalid-auc:0.676974\n",
      "[2400]\ttrain-auc:0.889296\tvalid-auc:0.677454\n",
      "[2600]\ttrain-auc:0.898769\tvalid-auc:0.676879\n",
      "Stopping. Best iteration:\n",
      "[2489]\ttrain-auc:0.89349\tvalid-auc:0.677549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(21, 22):\n",
    "    gc.collect()\n",
    "    xgb_train,xgb_val,xgb_tar_train,xgb_tar_val = train_test_split(train_words, train_target, test_size=0.15, random_state=i,\n",
    "                                                                   stratify=train_target)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(xgb_train, label=xgb_tar_train)\n",
    "    dval   = xgb.DMatrix(xgb_val, label=xgb_tar_val)\n",
    "    watchlist = [(dtrain, 'train'), (dval, 'valid')]\n",
    "    xbg_model = xgb.train(params_xgb, dtrain, 5000,  watchlist, maximize=True, verbose_eval=200, early_stopping_rounds=200)\n",
    "    xgb_val1 = xbg_model.predict(dval)\n",
    "    xgb_pred = xbg_model.predict(xgb.DMatrix(test_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Would be a good idea to mix numerical and categorical columns with boosting \\\n",
    "2.Stack NN with lgb , xgb and use the metamodel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
