{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import pylab as pl\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.sparse import hstack\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10001)\n",
    "import random\n",
    "import tensorflow as tf\n",
    "random.seed(10002)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=6, inter_op_parallelism_threads=5)\n",
    "from keras import backend\n",
    "tf.set_random_seed(10003)\n",
    "backend.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_raw_data=pd.read_csv(\"../../../Users/sreek/Documents/vg_donors_choose/train/train.csv\")\n",
    "test_raw_data=pd.read_csv(\"../../../Users/sreek/Documents/vg_donors_choose/test/test.csv\",low_memory=False)\n",
    "res_raw_data=pd.read_csv(\"../../../Users/sreek/Documents/vg_donors_choose/resources/resources.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182080, 16)\n",
      "(78035, 15)\n",
      "(1541272, 4)\n"
     ]
    }
   ],
   "source": [
    "print (train_raw_data.shape)\n",
    "print(test_raw_data.shape)\n",
    "print (res_raw_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_data(dataF):\n",
    "    \n",
    "    dataF.loc[dataF.project_essay_4.isnull(), ['project_essay_4','project_essay_2']] = \\\n",
    "    dataF.loc[dataF.project_essay_4.isnull(), ['project_essay_2','project_essay_4']].values\n",
    "\n",
    "    dataF[['project_essay_2','project_essay_3']] = dataF[['project_essay_2','project_essay_3']].fillna('')\n",
    "\n",
    "    dataF['project_essay_1'] = dataF.apply(lambda row: ' '.join([str(row['project_essay_1']), \n",
    "                                                     str(row['project_essay_2'])]), axis=1)\n",
    "    dataF['project_essay_2'] = dataF.apply(lambda row: ' '.join([str(row['project_essay_3']),\n",
    "                                                     str(row['project_essay_4'])]), axis=1)\n",
    "\n",
    "    dataF = dataF.drop(['project_essay_3', 'project_essay_4'], axis=1,inplace=True)\n",
    "\n",
    "\n",
    "cleanup_data(train_raw_data)\n",
    "cleanup_data(test_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_raw_data['Total'] = res_raw_data['quantity']*res_raw_data['price']\n",
    "res_data = res_raw_data.groupby('id').agg({'description':'count',\n",
    "                            'quantity':'sum',\n",
    "                            'price':'sum',\n",
    "                            'Total':'sum'}).rename(columns={'description':'items'})\n",
    "res_data['avgPrice'] = res_data.Total / res_data.quantity\n",
    "numFeatures = ['items', 'quantity', 'price', 'Total', 'avgPrice']\n",
    "\n",
    "for func in ['min', 'max', 'mean']:\n",
    "    res_data = res_data.join(res_raw_data.groupby('id').agg({'quantity':func,\n",
    "                                          'price':func,\n",
    "                                          'Total':func}).rename(\n",
    "                                columns={'quantity':func+'Quantity',\n",
    "                                         'price':func+'Price',\n",
    "                                         'Total':func+'Total'}).fillna(0))\n",
    "    numFeatures += [func+'Quantity', func+'Price', func+'Total']\n",
    "\n",
    "res_data = res_data.join(res_raw_data.groupby('id').agg(\n",
    "    {'description':lambda x:' '.join(x.values.astype(str))}).rename(\n",
    "    columns={'description':'resource_description'}))\n",
    "\n",
    "train_data = train_raw_data.join(res_data, on='id')\n",
    "test_data=test_raw_data.join(res_data,on='id')\n",
    "\n",
    "\n",
    "train_data['price_category'] = pl.digitize(train_data.Total, [0, 50, 100, 250, 500, 1000, pl.inf])\n",
    "test_data['price_category'] = pl.digitize(test_data.Total, [0, 50, 100, 250, 500, 1000, pl.inf])\n",
    "numFeatures.append('price_category')\n",
    "\n",
    "for c in ['Quantity', 'Price', 'Total']:\n",
    "    train_data['max%s_min%s'%(c,c)] = train_data['max%s'%c] - train_data['min%s'%c]\n",
    "    test_data['max%s_min%s'%(c,c)] = test_data['max%s'%c] - test_data['min%s'%c]\n",
    "    numFeatures.append('max%s_min%s'%(c,c))\n",
    "\n",
    "del res_data, train_raw_data, res_raw_data, test_raw_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data['teacher_id'] = LabelEncoder().fit_transform(train_data['teacher_id'])\n",
    "train_data['teacher_gender_unknown'] = train_data.teacher_prefix.apply(lambda x:int(x not in ['Ms.', 'Mrs.', 'Mr.']))  \n",
    "test_data['teacher_id'] = LabelEncoder().fit_transform(test_data['teacher_id'])\n",
    "test_data['teacher_gender_unknown'] = test_data.teacher_prefix.apply(lambda x:int(x not in ['Ms.', 'Mrs.', 'Mr.']))  \n",
    "aggFtrs=[]    \n",
    "       \n",
    "for col in ['school_state', 'teacher_id', 'teacher_prefix', 'teacher_gender_unknown', 'project_grade_category', \n",
    "                'project_subject_categories', 'project_subject_subcategories', 'teacher_number_of_previously_posted_projects']:\n",
    "        Aggtr = train_data[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_agg'})\n",
    "        Aggts = test_data[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_agg'})\n",
    "        Aggtr /= Aggtr.sum()\n",
    "        Aggts /= Aggts.sum()\n",
    "        train_data = train_data.join(Aggtr, on=col)\n",
    "        test_data = test_data.join(Aggts, on=col) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numFeatures += ['teacher_number_of_previously_posted_projects','teacher_id','teacher_gender_unknown']\n",
    "aggFtrs+=['school_state_agg', 'teacher_id_agg', 'teacher_prefix_agg', 'teacher_gender_unknown_agg', \\\n",
    "          'project_grade_category_agg', \n",
    "          'project_subject_categories_agg', 'project_subject_subcategories_agg',\\\n",
    "          'teacher_number_of_previously_posted_projects_agg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTimeFeatures(data):\n",
    "    data['year'] = data['project_submitted_datetime'].apply(lambda x: x.year)\n",
    "    data['month'] = data['project_submitted_datetime'].apply(lambda x: x.month)\n",
    "    data['day'] = data['project_submitted_datetime'].apply(lambda x: x.day)\n",
    "    data['dow'] = data['project_submitted_datetime'].apply(lambda x: x.dayofweek)\n",
    "    return data\n",
    "train_data['project_submitted_datetime'] = pd.to_datetime(train_data['project_submitted_datetime'])\n",
    "test_data['project_submitted_datetime'] = pd.to_datetime(test_data['project_submitted_datetime'])\n",
    "getTimeFeatures(train_data)\n",
    "getTimeFeatures(test_data)\n",
    "\n",
    "for col in  ['year', 'month', 'day', 'dow']:\n",
    "        Aggtr = train_data[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_agg'})\n",
    "        Aggts = test_data[['id', col]].groupby(col).agg('count').rename(columns={'id':col+'_agg'})\n",
    "        Aggtr /= Aggtr.sum()\n",
    "        Aggts /= Aggts.sum()\n",
    "        train_data = train_data.join(Aggtr, on=col)\n",
    "        test_data = test_data.join(Aggts, on=col)\n",
    "            \n",
    "\n",
    "aggFtrs+=['year_agg', 'month_agg', 'day_agg', 'dow_agg']\n",
    "numFeatures += ['year', 'month', 'day', 'dow']\n",
    "numFeatures += aggFtrs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_features = ['teacher_prefix','school_state','project_grade_category',\\\n",
    "                'project_subject_categories','project_subject_subcategories']\n",
    "cat_features_hash = [col+\"_hash\" for col in cat_features]\n",
    "max_size=15000#0\n",
    "def feature_hash(df, max_size=max_size):\n",
    "    for col in cat_features:\n",
    "        df[col+\"_hash\"] = df[col].apply(lambda x: hash(x)%max_size)\n",
    "    return df\n",
    "\n",
    "train_data = feature_hash(train_data)\n",
    "test_data = feature_hash(test_data)\n",
    "\n",
    "train_cat_new = np.array(train_data[cat_features_hash], dtype=np.int)\n",
    "test_cat_new = np.array(test_data[cat_features_hash], dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pprocess(string):\n",
    "\n",
    "    string = re.sub(r'(\\\")', ' ', string)\n",
    "    string = re.sub(r'(\\r)', ' ', string)\n",
    "    string = re.sub(r'(\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\r\\n)', ' ', string)\n",
    "    string = re.sub(r'(\\\\)', ' ', string)\n",
    "    string = re.sub(r'\\t', ' ', string)\n",
    "    string = re.sub(r'\\:', ' ', string)\n",
    "    string = re.sub(r'\\\"\\\"\\\"\\\"', ' ', string)\n",
    "    string = re.sub(r'_', ' ', string)\n",
    "    string = re.sub(r'\\+', ' ', string)\n",
    "    string = re.sub(r'\\=', ' ', string)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_cols=['project_essay_1','project_essay_2','project_resource_summary','resource_description','project_title']\n",
    "train_data['text'] = train_data.apply(lambda x: \" \".join(x[col] for col in text_cols), axis=1)\n",
    "test_data['text'] = test_data.apply(lambda x: \" \".join(x[col] for col in text_cols), axis=1)                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['text']=train_data['text'].apply(pprocess)\n",
    "test_data['text']=test_data['text'].apply(pprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "porter = PorterStemmer()\n",
    "def filter_stop_words(sentence):\n",
    "    new_sent = [porter.stem(word.lower()) for word in sentence.split() if word not in stop_words]\n",
    "    new_sent=  [\"\".join(sent) for sent in new_sent]\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['text']=train_data['text'].apply(filter_stop_words)\n",
    "test_data['text']=test_data['text'].apply(filter_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 100000\n",
    "from keras.preprocessing import text,sequence\n",
    "tokenizer = text.Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "tokenizer.fit_on_texts(train_data['text'].str.join(' ').tolist()+test_data['text'].str.join(' ').tolist())\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_text_data_list=tokenizer.texts_to_sequences(train_data[\"text\"].str.join(' ').tolist())\n",
    "ta_text_data_list=tokenizer.texts_to_sequences(test_data[\"text\"].str.join(' ').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_words = sequence.pad_sequences(tr_text_data_list, maxlen=300)\n",
    "test_words = sequence.pad_sequences(ta_text_data_list, maxlen=300)\n",
    "\n",
    "train_target = train_data.project_is_approved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../../../Users/sreek/Documents/vg_donors_choose/crawl-300d-2M.vec/crawl-300d-2M.vec'\n",
    "embed_size=300\n",
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten, concatenate, Dropout, Convolution1D, \\\n",
    "GlobalMaxPool1D,SpatialDropout1D,Bidirectional,PReLU,GRU\n",
    "from keras.models import Model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train_data_num=StandardScaler().fit_transform(train_data[numFeatures].fillna(0))\n",
    "test_data_num=StandardScaler().fit_transform(test_data[numFeatures].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_NN_model():\n",
    "    cat_input = Input((len(cat_features_hash), ))\n",
    "    num_input = Input((len(numFeatures), ))\n",
    "    text_input = Input((300, ))\n",
    "    m_cat=Embedding(max_size,10)(cat_input)\n",
    "    m_cat=SpatialDropout1D(0.3)(m_cat)\n",
    "    m_cat = Flatten()(m_cat)\n",
    "    m_words = Embedding(max_features, 300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)(text_input)\n",
    "    m_words = SpatialDropout1D(0.3)(m_words)\n",
    "    m_words =Bidirectional(GRU(50, return_sequences=True))(m_words)\n",
    "    m_words = Convolution1D(100, 3, activation=\"relu\")(m_words)\n",
    "    m_words = GlobalMaxPool1D()(m_words)\n",
    "    m_cat = Dense(100, activation=\"relu\")(m_cat)\n",
    "    m_num = Dense(100, activation=\"relu\")(num_input)\n",
    "    m = concatenate([m_cat, m_num, m_words])\n",
    "    m = Dense(50, activation=\"relu\")(m)\n",
    "    m = Dropout(0.25)(m)\n",
    "    predictions = Dense(1, activation=\"sigmoid\")(m)\n",
    "    model = Model(inputs=[cat_input, num_input, text_input], outputs=predictions)\n",
    "    model.compile(optimizer=optimizers.Adam(0.0005, decay=1e-6),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 163872 samples, validate on 18208 samples\n",
      "Epoch 1/5\n",
      "Epoch 00000: val_loss improved from inf to 0.37153, saving model to RNN.h5\n",
      "448s - loss: 0.4015 - acc: 0.8468 - val_loss: 0.3715 - val_acc: 0.8509\n",
      "Epoch 2/5\n",
      "Epoch 00001: val_loss improved from 0.37153 to 0.35445, saving model to RNN.h5\n",
      "442s - loss: 0.3717 - acc: 0.8514 - val_loss: 0.3545 - val_acc: 0.8573\n",
      "Epoch 3/5\n",
      "Epoch 00002: val_loss improved from 0.35445 to 0.34915, saving model to RNN.h5\n",
      "444s - loss: 0.3568 - acc: 0.8564 - val_loss: 0.3492 - val_acc: 0.8591\n",
      "Epoch 4/5\n",
      "Epoch 00003: val_loss improved from 0.34915 to 0.34860, saving model to RNN.h5\n",
      "442s - loss: 0.3507 - acc: 0.8593 - val_loss: 0.3486 - val_acc: 0.8591\n",
      "Epoch 5/5\n",
      "Epoch 00004: val_loss improved from 0.34860 to 0.34573, saving model to RNN.h5\n",
      "448s - loss: 0.3456 - acc: 0.8612 - val_loss: 0.3457 - val_acc: 0.8603\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "file_path='RNN.h5'\n",
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=2, save_best_only=True, save_weights_only=True,\n",
    "                                     mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=4)\n",
    "lr_reduced = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=2,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='min')\n",
    "callbacks_list = [checkpoint, early, lr_reduced]\n",
    "model=build_NN_model()\n",
    "history = model.fit([train_cat_new, train_data_num, train_words], train_target, validation_split=0.1,\n",
    "                    verbose=2,callbacks=callbacks_list,\n",
    "          epochs=5, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[2000,300,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bidirectional_1/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](spatial_dropout1d_2/cond/Merge)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: dense_4/Sigmoid/_219 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_598_dense_4/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'bidirectional_1/zeros_like', defined at:\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 596, in launch_instance\n    app.start()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-087151803252>\", line 16, in <module>\n    model=build_NN_model()\n  File \"<ipython-input-27-c2513483fd55>\", line 12, in build_NN_model\n    m_words =Bidirectional(GRU(50, return_sequences=True))(m_words)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\engine\\topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\layers\\wrappers.py\", line 294, in call\n    y = self.forward_layer.call(inputs, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 315, in call\n    initial_state = self.get_initial_state(inputs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 244, in get_initial_state\n    initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 704, in zeros_like\n    return tf.zeros_like(x, dtype=dtype, name=name)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1681, in zeros_like\n    return gen_array_ops.zeros_like(tensor, name=name)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 11442, in zeros_like\n    \"ZerosLike\", x=x, name=name)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[2000,300,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bidirectional_1/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](spatial_dropout1d_2/cond/Merge)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: dense_4/Sigmoid/_219 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_598_dense_4/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[2000,300,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bidirectional_1/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](spatial_dropout1d_2/cond/Merge)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: dense_4/Sigmoid/_219 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_598_dense_4/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c26b47e2fa45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mtrain_cat_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_cat_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"project_is_approved\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'project_is_approved'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gru_cnn_submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1711\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[1;32m-> 1713\u001b[1;33m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[0;32m   1714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1267\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1270\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[2000,300,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bidirectional_1/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](spatial_dropout1d_2/cond/Merge)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: dense_4/Sigmoid/_219 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_598_dense_4/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'bidirectional_1/zeros_like', defined at:\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 596, in launch_instance\n    app.start()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-087151803252>\", line 16, in <module>\n    model=build_NN_model()\n  File \"<ipython-input-27-c2513483fd55>\", line 12, in build_NN_model\n    m_words =Bidirectional(GRU(50, return_sequences=True))(m_words)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\engine\\topology.py\", line 602, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\layers\\wrappers.py\", line 294, in call\n    y = self.forward_layer.call(inputs, **kwargs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 315, in call\n    initial_state = self.get_initial_state(inputs)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 244, in get_initial_state\n    initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 704, in zeros_like\n    return tf.zeros_like(x, dtype=dtype, name=name)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1681, in zeros_like\n    return gen_array_ops.zeros_like(tensor, name=name)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 11442, in zeros_like\n    \"ZerosLike\", x=x, name=name)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\python\\pyLibs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[2000,300,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: bidirectional_1/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](spatial_dropout1d_2/cond/Merge)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: dense_4/Sigmoid/_219 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_598_dense_4/Sigmoid\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "del train_cat_new, train_data_num, train_words,train_target\n",
    "model.load_weights(file_path)\n",
    "pred_test = model.predict([test_cat_new, test_data_num, test_words], batch_size=2000)\n",
    "test[\"project_is_approved\"] = pred_test\n",
    "test[['id', 'project_is_approved']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "        'eta': 0.05,\n",
    "        'max_depth': 4,\n",
    "        'subsample': 0.85,\n",
    "        'colsample_bytree': 0.25,\n",
    "        'min_child_weight': 3,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'seed': 0,\n",
    "        'silent': 1,\n",
    "    }\n",
    "params_lgb = {\n",
    "        'boosting_type': 'dart',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.25,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'seed': 0,\n",
    "        'verbose': 0,\n",
    "    }\n",
    "for i in range(21, 22):\n",
    "    gc.collect()\n",
    "    X_train, X_val, Tar_train, Tar_val = train_test_split(Xtr, Ttr_tar, test_size=0.15, random_state=i, stratify=Ttr_tar)\n",
    "\n",
    "    # LGB\n",
    "    dtrain = lgb.Dataset(X_train, Tar_train)\n",
    "    dval   = lgb.Dataset(X_val, Tar_val)\n",
    "    model = lgb.train(params_lgb, dtrain, num_boost_round=10000, valid_sets=[dtrain, dval], early_stopping_rounds=200, verbose_eval=200)\n",
    "    Yvl2 = model.predict(X_val)\n",
    "    Yts2 = model.predict(Xts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
